{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "import pandas as pd\n",
    "import os  \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import plotly as plt\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"/project/MERGED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 251 entries, 0 to 250\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                              Non-Null Count  Dtype  \n",
      "---  ------                                              --------------  -----  \n",
      " 0   Brief Name                                          251 non-null    object \n",
      " 1   Client                                              251 non-null    object \n",
      " 2   Shortlisted Agency                                  251 non-null    object \n",
      " 3   Agency Skills                                       210 non-null    object \n",
      " 4   Agency Industries                                   210 non-null    object \n",
      " 5   Service Description                                 220 non-null    object \n",
      " 6   Agency Description                                  226 non-null    object \n",
      " 7   Agency services                                     226 non-null    object \n",
      " 8   Other Services                                      155 non-null    object \n",
      " 9   Vision                                              124 non-null    object \n",
      " 10  Industry Experience                                 121 non-null    object \n",
      " 11  Agency Locations                                    235 non-null    object \n",
      " 12  Brief Region                                        251 non-null    object \n",
      " 13  Status                                              251 non-null    object \n",
      " 14  Winning Agency                                      251 non-null    object \n",
      " 15  Number of Agencies Shortlisted Brief                251 non-null    int64  \n",
      " 16  Client_WA_Interaction                               251 non-null    float64\n",
      " 17  Project extension with same agency                  251 non-null    int64  \n",
      " 18  Client selected agency themselves from marketplace  251 non-null    float64\n",
      " 19  Right geography / time zone                         251 non-null    int64  \n",
      " 20  Strong skill set match                              251 non-null    int64  \n",
      " 21  Deep / relevant sector expertise                    251 non-null    int64  \n",
      " 22  Good cultural fit                                   251 non-null    int64  \n",
      " 23  Right price level                                   251 non-null    int64  \n",
      " 24  Strong track record                                 251 non-null    int64  \n",
      " 25  Match Score                                         251 non-null    float64\n",
      " 26  Lead_George Patten                                  251 non-null    int64  \n",
      " 27  Lead_Gideon Hyde                                    251 non-null    int64  \n",
      " 28  Lead_Hannah Fraser                                  251 non-null    int64  \n",
      " 29  Lead_Kate Walker                                    251 non-null    int64  \n",
      " 30  Lead_Paul Bowman                                    251 non-null    int64  \n",
      " 31  Lead_Peter Sayburn                                  251 non-null    int64  \n",
      " 32  Lead_Phil Kohler                                    251 non-null    int64  \n",
      " 33  Lead_Robin Scarborough                              251 non-null    int64  \n",
      " 34  Sub Brief Taxonomy                                  250 non-null    object \n",
      " 35  Main Brief Taxonomy                                 250 non-null    object \n",
      "dtypes: float64(3), int64(16), object(17)\n",
      "memory usage: 70.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 251 entries, 0 to 250\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                              Non-Null Count  Dtype  \n",
      "---  ------                                              --------------  -----  \n",
      " 0   Brief Name                                          251 non-null    object \n",
      " 1   Client                                              251 non-null    object \n",
      " 2   Shortlisted Agency                                  251 non-null    object \n",
      " 3   Agency Skills                                       210 non-null    object \n",
      " 4   Agency Industries                                   210 non-null    object \n",
      " 5   Service Description                                 220 non-null    object \n",
      " 6   Agency Description                                  226 non-null    object \n",
      " 7   Agency services                                     226 non-null    object \n",
      " 8   Other Services                                      155 non-null    object \n",
      " 9   Vision                                              124 non-null    object \n",
      " 10  Industry Experience                                 121 non-null    object \n",
      " 11  Agency Locations                                    235 non-null    object \n",
      " 12  Brief Region                                        251 non-null    object \n",
      " 13  Status                                              251 non-null    object \n",
      " 14  Winning Agency                                      251 non-null    object \n",
      " 15  Project extension with same agency                  251 non-null    int64  \n",
      " 16  Client selected agency themselves from marketplace  251 non-null    float64\n",
      " 17  Right geography / time zone                         251 non-null    int64  \n",
      " 18  Strong skill set match                              251 non-null    int64  \n",
      " 19  Deep / relevant sector expertise                    251 non-null    int64  \n",
      " 20  Good cultural fit                                   251 non-null    int64  \n",
      " 21  Right price level                                   251 non-null    int64  \n",
      " 22  Strong track record                                 251 non-null    int64  \n",
      " 23  Sub Brief Taxonomy                                  250 non-null    object \n",
      " 24  Main Brief Taxonomy                                 250 non-null    object \n",
      "dtypes: float64(1), int64(7), object(17)\n",
      "memory usage: 49.1+ KB\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    \"Match Score\",\n",
    "    \"Lead_George Patten\",\n",
    "    \"Lead_Gideon Hyde\",\n",
    "    \"Lead_Hannah Fraser\",\n",
    "    \"Lead_Kate Walker\",\n",
    "    \"Lead_Paul Bowman\",\n",
    "    \"Lead_Peter Sayburn\",\n",
    "    \"Lead_Phil Kohler\",\n",
    "    \"Lead_Robin Scarborough\",\n",
    "    \"Number of Agencies Shortlisted Brief\", \n",
    "    \"Client_WA_Interaction\"\n",
    "]\n",
    "\n",
    "df_all = df_all.drop(columns=columns_to_drop)\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT BASED FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: tfidf vectorization, using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping nans for certain columns\n",
    "columns_to_check = ['Agency Skills', 'Agency Industries']\n",
    "df = df.dropna(subset=columns_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split unique brief names into training and test sets\n",
    "unique_brief_names = df['Brief Name'].unique()\n",
    "train_brief_names, test_brief_names = train_test_split(unique_brief_names, \n",
    "                                                       test_size=0.2, \n",
    "                                                       random_state=42)\n",
    "\n",
    "# Divide the dataframe into training and test dataframes based on the brief names\n",
    "train_df = df[df['Brief Name'].isin(train_brief_names)]\n",
    "test_df = df[df['Brief Name'].isin(test_brief_names)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split unique brief names into training and test sets\n",
    "unique_brief_names = df['Brief Name'].unique()\n",
    "train_brief_names, test_brief_names = train_test_split(unique_brief_names, \n",
    "                                                       test_size=0.2, \n",
    "                                                       random_state=42)\n",
    "\n",
    "# Divide the dataframe into training and test dataframes based on the brief names\n",
    "train_df = df[df['Brief Name'].isin(train_brief_names)]\n",
    "test_df = df[df['Brief Name'].isin(test_brief_names)]\n",
    "\n",
    "# Create a function to concatenate the agency features\n",
    "def concat_features(x):\n",
    "    return ' '.join(x['Agency Skills'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Agency Industries'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Service Description'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Agency Description'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Agency services'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Other Services'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Vision'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Industry Experience'].astype(str)) + ' ' + \\\n",
    "           ' '.join(x['Agency Locations'].astype(str))\n",
    "\n",
    "# Group by 'Brief Name' and concatenate the other features\n",
    "brief_features = df.groupby('Brief Name')[['Brief Region', \n",
    "                                           'Sub Brief Taxonomy',\n",
    "                                           'Main Brief Taxonomy']].apply(\n",
    "    lambda x: ' '.join(x['Brief Region'].astype(str)) + ' ' +\n",
    "              ' '.join(x['Sub Brief Taxonomy'].astype(str)) + ' ' +\n",
    "              ' '.join(x['Main Brief Taxonomy'].astype(str))\n",
    ").reset_index(name='combined_features')\n",
    "\n",
    "# Function to process the data\n",
    "def process_data(dataframe):\n",
    "    agency_features = dataframe.groupby('Shortlisted Agency').apply(concat_features).reset_index(name='combined_features')\n",
    "    brief_features = dataframe.groupby('Brief Name')[['Brief Region', 'Sub Brief Taxonomy', 'Main Brief Taxonomy']].apply(\n",
    "        lambda x: ' '.join(x['Brief Region'].astype(str)) + ' ' +\n",
    "                  ' '.join(x['Sub Brief Taxonomy'].astype(str)) + ' ' +\n",
    "                  ' '.join(x['Main Brief Taxonomy'].astype(str))\n",
    "    ).reset_index(name='combined_features')\n",
    "\n",
    "    return agency_features, brief_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the training and test data\n",
    "train_agency_features, train_brief_features = process_data(train_df)\n",
    "test_agency_features, test_brief_features = process_data(test_df)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the training agency features and transform both training and test agency features\n",
    "agency_matrix_train = tfidf_vectorizer.fit_transform(train_agency_features['combined_features'])\n",
    "agency_matrix_test = tfidf_vectorizer.transform(test_agency_features['combined_features'])\n",
    "\n",
    "# Transform the training and test brief features\n",
    "brief_matrix_train = tfidf_vectorizer.transform(train_brief_features['combined_features'])\n",
    "brief_matrix_test = tfidf_vectorizer.transform(test_brief_features['combined_features'])\n",
    "\n",
    "# Calculate similarity matrices for training and test data\n",
    "similarity_matrix_train = cosine_similarity(brief_matrix_train, agency_matrix_train)\n",
    "similarity_matrix_test = cosine_similarity(brief_matrix_test, agency_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recommendation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(similarity_matrix, agency_names, top_n=10):\n",
    "    recommendations = []\n",
    "    for similarity_scores in similarity_matrix:\n",
    "        # Get the indexes of the top-N similar agencies\n",
    "        similar_agencies_idx = similarity_scores.argsort()[::-1][:top_n]\n",
    "        recommendations.append([agency_names[idx] for idx in similar_agencies_idx])\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "agency_names_train = train_agency_features['Shortlisted Agency'].values\n",
    "agency_names_test = test_agency_features['Shortlisted Agency'].values\n",
    "\n",
    "recommendations_test = get_recommendations(similarity_matrix_test, agency_names_test)\n",
    "\n",
    "# Get the actual shortlisted agencies for the test brief names\n",
    "actual_shortlisted_agencies_test = [test_df[test_df['Brief Name'] == name]['Shortlisted Agency'].tolist() for name in test_brief_features['Brief Name']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for evaluation metrics \n",
    "# Evaluation Metrics\n",
    "def evaluate_recommendations(recommendations, actual_shortlisted_agencies):\n",
    "    total_precision = 0\n",
    "    for i, recs in enumerate(recommendations):\n",
    "        true_positives = len(set(recs) & set(actual_shortlisted_agencies[i]))\n",
    "        precision = true_positives / len(recs)\n",
    "        total_precision += precision\n",
    "    return total_precision / len(recommendations)\n",
    "\n",
    "\n",
    "def calculate_recall(recommendations, actual_shortlisted_agencies):\n",
    "    total_recall = 0\n",
    "    for i, recs in enumerate(recommendations):\n",
    "        true_positives = len(set(recs) & set(actual_shortlisted_agencies[i]))\n",
    "        recall = true_positives / len(actual_shortlisted_agencies[i])\n",
    "        total_recall += recall\n",
    "    return total_recall / len(recommendations)\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_mean_average_precision(recommendations, actual_shortlisted_agencies):\n",
    "    total_map = 0\n",
    "    for i, recs in enumerate(recommendations):\n",
    "        precisions = []\n",
    "        relevant_count = 0\n",
    "        for j, rec in enumerate(recs):\n",
    "            if rec in set(actual_shortlisted_agencies[i]):\n",
    "                relevant_count += 1\n",
    "                precisions.append(relevant_count / (j + 1))\n",
    "        avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "        total_map += avg_precision\n",
    "    return total_map / len(recommendations)\n",
    "\n",
    "\n",
    "def calculate_ndcg(recommendations, actual_shortlisted_agencies, p=10):\n",
    "    total_ndcg = 0\n",
    "    for i, recs in enumerate(recommendations):\n",
    "        dcg = 0\n",
    "        for j, rec in enumerate(recs[:p]):\n",
    "            if rec in set(actual_shortlisted_agencies[i]):\n",
    "                dcg += 1 / np.log2(j + 2)\n",
    "        idcg = sum([1 / np.log2(i + 2) for i in range(min(len(actual_shortlisted_agencies[i]), p))])\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        total_ndcg += ndcg\n",
    "    return total_ndcg / len(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.11600000000000003\n",
      "Test Recall: 0.8133333333333335\n",
      "Test F1-Score: 0.20304160688665715\n",
      "Test MAP: 0.46115873015873016\n",
      "Test NDCG: 0.528932667933488\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "precision_test = evaluate_recommendations(recommendations_test, actual_shortlisted_agencies_test)\n",
    "recall_test = calculate_recall(recommendations_test, actual_shortlisted_agencies_test)\n",
    "f1_score_test = calculate_f1_score(precision_test, recall_test)\n",
    "mean_avg_precision_test = calculate_mean_average_precision(recommendations_test, actual_shortlisted_agencies_test)\n",
    "ndcg_test = calculate_ndcg(recommendations_test, actual_shortlisted_agencies_test)\n",
    "\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)\n",
    "print(\"Test F1-Score:\", f1_score_test)\n",
    "print(\"Test MAP:\", mean_avg_precision_test)\n",
    "print(\"Test NDCG:\", ndcg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing for a brief "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for brief name: Awards applications x5\n",
      "Agency: Five by Five Global, Similarity Score: 0.36258736423024607\n",
      "Agency: Boost Awards, Similarity Score: 0.19252330615766763\n",
      "Agency: Innovate Live, Similarity Score: 0.11780251868472916\n",
      "Agency: Inc, Similarity Score: 0.06942974755056366\n",
      "Agency: Balmer Agency, Similarity Score: 0.06522322277056924\n",
      "Agency: In Marketing We Trust, Similarity Score: 0.05182307605955247\n",
      "Agency: HACKMASTERS, Similarity Score: 0.046888524072648013\n",
      "Agency: Reed Words, Similarity Score: 0.03773793218185841\n",
      "Agency: I-AM, Similarity Score: 0.03533520257639661\n",
      "Agency: xDesign, Similarity Score: 0.03488483246766551\n"
     ]
    }
   ],
   "source": [
    "agency_names_test = test_agency_features['Shortlisted Agency'].values\n",
    "\n",
    "def get_single_recommendation(brief_name, brief_features, similarity_matrix, agency_names, top_n=10):\n",
    "    # Find the index of the brief name\n",
    "    brief_idx = brief_features[brief_features['Brief Name'] == brief_name].index[0]\n",
    "    \n",
    "    # Get similarity scores and recommended agency indexes\n",
    "    similarity_scores = similarity_matrix[brief_idx]\n",
    "    similar_agencies_idx = similarity_scores.argsort()[::-1][:top_n]\n",
    "    \n",
    "    # Get the names and scores of the recommended agencies\n",
    "    recommended_agencies = [agency_names[idx] for idx in similar_agencies_idx]\n",
    "    recommended_scores = [similarity_scores[idx] for idx in similar_agencies_idx]\n",
    "    \n",
    "    return recommended_agencies, recommended_scores\n",
    "\n",
    "# Example usage\n",
    "brief_name = test_brief_features['Brief Name'].iloc[0]\n",
    "recommended_agencies, recommended_scores = get_single_recommendation(brief_name, test_brief_features, similarity_matrix_test, agency_names_test)\n",
    "\n",
    "print(\"Recommendations for brief name:\", brief_name)\n",
    "for agency, score in zip(recommended_agencies, recommended_scores):\n",
    "    print(f\"Agency: {agency}, Similarity Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2- Model with encoding for categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilabel binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the columns that need MultiLabelBinarizer\n",
    "multilabel_columns = ['Agency Skills', 'Agency Industries', 'Sub Brief Taxonomy',\n",
    "                      'Main Brief Taxonomy']\n",
    "\n",
    "# Apply MultiLabelBinarizer to these columns\n",
    "mlb = MultiLabelBinarizer()\n",
    "for column in multilabel_columns:\n",
    "    def process_value(x):\n",
    "        if isinstance(x, str):\n",
    "            return tuple(str(val).strip() for val in x.strip('()').split(','))\n",
    "        else:\n",
    "            return ()\n",
    "    \n",
    "    df1[column] = df[column].apply(process_value)\n",
    "    mlb_matrix = mlb.fit_transform(df1[column])\n",
    "    mlb_df = pd.DataFrame(mlb_matrix, columns=[f\"{column}_{class_}\" for class_ in mlb.classes_])\n",
    "    df1 = pd.concat([df1, mlb_df], axis=1)\n",
    "    df1 = df1.drop(column, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding for location features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Agency Locations': ['United Kingdom' 'United Kingdom, United States'\n",
      " 'Australia, United Kingdom, United States' 'Australia' nan\n",
      " 'Australia, United Kingdom' 'Australia, United States' 'United States']\n",
      "Unique values in 'Brief Region': ['united kingdom' 'australia' 'united states']\n",
      "Unique values in 'Agency Locations' after removing white spaces: ['United Kingdom' 'United States' 'Australia' nan]\n",
      "Unique values in 'Brief Region' after removing white spaces: ['united kingdom' 'australia' 'united states']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Brief Name</th>\n",
       "      <th>Client</th>\n",
       "      <th>Shortlisted Agency</th>\n",
       "      <th>Service Description</th>\n",
       "      <th>Agency Description</th>\n",
       "      <th>Agency services</th>\n",
       "      <th>Other Services</th>\n",
       "      <th>Vision</th>\n",
       "      <th>Industry Experience</th>\n",
       "      <th>...</th>\n",
       "      <th>Main Brief Taxonomy_Spatial_Design)</th>\n",
       "      <th>Main Brief Taxonomy_Tech &amp; Development</th>\n",
       "      <th>Main Brief Taxonomy_Tech &amp; Development)</th>\n",
       "      <th>Main Brief Taxonomy_User_Experience/User_Interface_Design</th>\n",
       "      <th>AgencyLocations_Australia</th>\n",
       "      <th>AgencyLocations_United Kingdom</th>\n",
       "      <th>AgencyLocations_United States</th>\n",
       "      <th>BriefRegion_australia</th>\n",
       "      <th>BriefRegion_united kingdom</th>\n",
       "      <th>BriefRegion_united states</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Awards applications x5</td>\n",
       "      <td>AXA</td>\n",
       "      <td>Boost Awards</td>\n",
       "      <td>We are the world’s first and largest award ent...</td>\n",
       "      <td>We are the world’s first and largest award ent...</td>\n",
       "      <td>(Public_Relations)</td>\n",
       "      <td>(Awards_consultancy)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Awards applications x5</td>\n",
       "      <td>AXA</td>\n",
       "      <td>Reed Words</td>\n",
       "      <td>Verbal branding and copy writing</td>\n",
       "      <td>We make brands and business stronger through l...</td>\n",
       "      <td>(Marketing_Planning), (Social_&amp;_Content)</td>\n",
       "      <td>(Copywriting), (Naming), (Tone_of_Voice), (Bra...</td>\n",
       "      <td>Making Brands and Businesses Stronger Through ...</td>\n",
       "      <td>FMCG,Retail,Non-Profit</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Awards applications x5</td>\n",
       "      <td>AXA</td>\n",
       "      <td>Reed Words</td>\n",
       "      <td>Verbal branding and copy writing</td>\n",
       "      <td>We make brands and business stronger through l...</td>\n",
       "      <td>(Marketing_Planning), (Social_&amp;_Content)</td>\n",
       "      <td>(Copywriting), (Naming), (Tone_of_Voice), (Bra...</td>\n",
       "      <td>Making Brands and Businesses Stronger Through ...</td>\n",
       "      <td>FMCG,Retail,Non-Profit</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Countrywide broker video</td>\n",
       "      <td>AXA</td>\n",
       "      <td>Viewpoint Studios</td>\n",
       "      <td>Advertising and commercial photography and film</td>\n",
       "      <td>Inspired photography and film</td>\n",
       "      <td>(Creative_&amp;_Production), (Social_&amp;_Content)</td>\n",
       "      <td>(Advertising_and_commercial_photography_and_film)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Countrywide broker video</td>\n",
       "      <td>AXA</td>\n",
       "      <td>Shoreditch Design Studio</td>\n",
       "      <td>The creative agency for your next big thing</td>\n",
       "      <td>The creative agency for your next big thing</td>\n",
       "      <td>(Strategic_Design), (UX/UI_Design)</td>\n",
       "      <td>(Product_Design), (Print_Design), (Animation),...</td>\n",
       "      <td>We create high quality digital design for busi...</td>\n",
       "      <td>Automotive,Fitness,Health,Insurance,Fintech,Te...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                Brief Name Client        Shortlisted Agency  \\\n",
       "0      0    Awards applications x5    AXA              Boost Awards   \n",
       "1      1    Awards applications x5    AXA                Reed Words   \n",
       "2      1    Awards applications x5    AXA                Reed Words   \n",
       "3      2  Countrywide broker video    AXA         Viewpoint Studios   \n",
       "4      3  Countrywide broker video    AXA  Shoreditch Design Studio   \n",
       "\n",
       "                                 Service Description  \\\n",
       "0  We are the world’s first and largest award ent...   \n",
       "1                  Verbal branding and copy writing    \n",
       "2                  Verbal branding and copy writing    \n",
       "3    Advertising and commercial photography and film   \n",
       "4        The creative agency for your next big thing   \n",
       "\n",
       "                                  Agency Description  \\\n",
       "0  We are the world’s first and largest award ent...   \n",
       "1  We make brands and business stronger through l...   \n",
       "2  We make brands and business stronger through l...   \n",
       "3                      Inspired photography and film   \n",
       "4        The creative agency for your next big thing   \n",
       "\n",
       "                               Agency services  \\\n",
       "0                           (Public_Relations)   \n",
       "1     (Marketing_Planning), (Social_&_Content)   \n",
       "2     (Marketing_Planning), (Social_&_Content)   \n",
       "3  (Creative_&_Production), (Social_&_Content)   \n",
       "4           (Strategic_Design), (UX/UI_Design)   \n",
       "\n",
       "                                      Other Services  \\\n",
       "0                               (Awards_consultancy)   \n",
       "1  (Copywriting), (Naming), (Tone_of_Voice), (Bra...   \n",
       "2  (Copywriting), (Naming), (Tone_of_Voice), (Bra...   \n",
       "3  (Advertising_and_commercial_photography_and_film)   \n",
       "4  (Product_Design), (Print_Design), (Animation),...   \n",
       "\n",
       "                                              Vision  \\\n",
       "0                                                NaN   \n",
       "1  Making Brands and Businesses Stronger Through ...   \n",
       "2  Making Brands and Businesses Stronger Through ...   \n",
       "3                                                NaN   \n",
       "4  We create high quality digital design for busi...   \n",
       "\n",
       "                                 Industry Experience  ...  \\\n",
       "0                                                NaN  ...   \n",
       "1                             FMCG,Retail,Non-Profit  ...   \n",
       "2                             FMCG,Retail,Non-Profit  ...   \n",
       "3                                                NaN  ...   \n",
       "4  Automotive,Fitness,Health,Insurance,Fintech,Te...  ...   \n",
       "\n",
       "  Main Brief Taxonomy_Spatial_Design) Main Brief Taxonomy_Tech & Development  \\\n",
       "0                                   0                                      0   \n",
       "1                                   0                                      0   \n",
       "2                                   0                                      0   \n",
       "3                                   0                                      0   \n",
       "4                                   0                                      0   \n",
       "\n",
       "   Main Brief Taxonomy_Tech & Development)  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   Main Brief Taxonomy_User_Experience/User_Interface_Design  \\\n",
       "0                                                  0           \n",
       "1                                                  0           \n",
       "2                                                  0           \n",
       "3                                                  0           \n",
       "4                                                  0           \n",
       "\n",
       "   AgencyLocations_Australia  AgencyLocations_United Kingdom  \\\n",
       "0                          0                               1   \n",
       "1                          0                               1   \n",
       "2                          0                               0   \n",
       "3                          0                               1   \n",
       "4                          1                               0   \n",
       "\n",
       "   AgencyLocations_United States  BriefRegion_australia  \\\n",
       "0                              0                      0   \n",
       "1                              0                      0   \n",
       "2                              1                      0   \n",
       "3                              0                      0   \n",
       "4                              0                      0   \n",
       "\n",
       "   BriefRegion_united kingdom  BriefRegion_united states  \n",
       "0                           1                          0  \n",
       "1                           1                          0  \n",
       "2                           1                          0  \n",
       "3                           1                          0  \n",
       "4                           1                          0  \n",
       "\n",
       "[5 rows x 324 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the unique values in 'Agency Locations' and 'Brief Region' columns\n",
    "print(\"Unique values in 'Agency Locations':\", df1['Agency Locations'].unique())\n",
    "print(\"Unique values in 'Brief Region':\", df1['Brief Region'].unique())\n",
    "\n",
    "# 'Agency Locations' column\n",
    "df1['Agency Locations'] = df1['Agency Locations'].str.split(', ')\n",
    "df1 = df1.explode('Agency Locations')\n",
    "\n",
    "# Remove trailing white spaces\n",
    "df1['Agency Locations'] = df1['Agency Locations'].str.strip()\n",
    "df1['Brief Region'] = df1['Brief Region'].str.strip()\n",
    "\n",
    "# Display the unique values after removing white spaces\n",
    "print(\"Unique values in 'Agency Locations' after removing white spaces:\", df1['Agency Locations'].unique())\n",
    "print(\"Unique values in 'Brief Region' after removing white spaces:\", df1['Brief Region'].unique())\n",
    "\n",
    "# One hot encode the exploded DataFrame, prefix the column names, and drop the original column\n",
    "one_hot_agency = pd.get_dummies(df1['Agency Locations'], prefix='AgencyLocations')\n",
    "df1 = pd.concat([df1.drop('Agency Locations', axis=1), one_hot_agency], axis=1)\n",
    "\n",
    "# Repeat the process for 'Brief Region'\n",
    "one_hot_brief = pd.get_dummies(df1['Brief Region'], prefix='BriefRegion')\n",
    "df1 = pd.concat([df1.drop('Brief Region', axis=1), one_hot_brief], axis=1)\n",
    "df1 = df1.reset_index()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split unique brief names into training and test sets\n",
    "unique_brief_names = df1['Brief Name'].unique()\n",
    "train_brief_names, test_brief_names = train_test_split(unique_brief_names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Divide the DataFrame into training and test DataFrames based on the brief names\n",
    "train_df = df1[df1['Brief Name'].isin(train_brief_names)]\n",
    "test_df = df1[df1['Brief Name'].isin(test_brief_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. similarity metric calculation uisng cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data to get agency and brief matrices\n",
    "def process_data(dataframe):\n",
    "    numeric_cols = dataframe.select_dtypes(include=['int64', 'float64']).columns.difference(['Brief Name', 'Shortlisted Agency'])\n",
    "    agency_features = dataframe.groupby('Shortlisted Agency')[numeric_cols].mean()\n",
    "    brief_features = dataframe.groupby('Brief Name')[numeric_cols].mean()\n",
    "\n",
    "    return agency_features, brief_features\n",
    "\n",
    "train_agency_matrix, train_brief_matrix = process_data(train_df)\n",
    "test_agency_matrix, test_brief_matrix = process_data(test_df)\n",
    "\n",
    "# Calculate similarity matrices for training and test data\n",
    "similarity_matrix_train = cosine_similarity(train_brief_matrix, train_agency_matrix)\n",
    "similarity_matrix_test = cosine_similarity(test_brief_matrix, test_agency_matrix)\n",
    "\n",
    "agency_names_test = test_agency_matrix.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recommendation generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for brief name: Acuity Magazine Podcast Series\n",
      "Agency: Ampel, Similarity Score: 1.0000000000000002\n",
      "Agency: Quill Peak Consulting, Similarity Score: 0.9995524492154881\n",
      "Agency: Bridge Studio, Similarity Score: 0.9995477694322612\n",
      "Agency: NMD+, Similarity Score: 0.9995075933367519\n",
      "Agency: Solution17, Similarity Score: 0.9994702770296426\n",
      "Agency: Rip + Tear, Similarity Score: 0.9994501085595292\n",
      "Agency: IE, Similarity Score: 0.9994242421233783\n",
      "Agency: Tobias, Similarity Score: 0.9993984235779853\n",
      "Agency: Dark Blue, Similarity Score: 0.999387326631536\n",
      "Agency: Hedgehog Lab, Similarity Score: 0.9993669581598749\n"
     ]
    }
   ],
   "source": [
    "def get_single_recommendation(brief_name, brief_features, similarity_matrix, agency_names, top_n=10):\n",
    "    # Find the index of the brief name\n",
    "    brief_idx = np.where(brief_features.index == brief_name)[0][0]\n",
    "\n",
    "    # Get similarity scores and recommended agency indexes\n",
    "    similarity_scores = similarity_matrix[brief_idx]\n",
    "    similar_agencies_idx = similarity_scores.argsort()[::-1][:top_n]\n",
    "\n",
    "    # Get the names and scores of the recommended agencies\n",
    "    recommended_agencies = [agency_names[idx] for idx in similar_agencies_idx]\n",
    "    recommended_scores = [similarity_scores[idx] for idx in similar_agencies_idx]\n",
    "\n",
    "    return recommended_agencies, recommended_scores\n",
    "\n",
    "# Example usage\n",
    "brief_name = test_brief_matrix.index[0]  # You can change this to any brief name in the test set\n",
    "recommended_agencies, recommended_scores = get_single_recommendation(brief_name, test_brief_matrix, similarity_matrix_test, agency_names_test)\n",
    "\n",
    "print(\"Recommendations for brief name:\", brief_name)\n",
    "for agency, score in zip(recommended_agencies, recommended_scores):\n",
    "    print(f\"Agency: {agency}, Similarity Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation generation for all the briefs in the test set \n",
    "def generate_recommendations(brief_features, similarity_matrix, agency_names, top_n=10):\n",
    "    recommendations = []\n",
    "    for brief_name in brief_features.index:\n",
    "        recommended_agencies, _ = get_single_recommendation(brief_name, brief_features, similarity_matrix, agency_names, top_n)\n",
    "        recommendations.append(recommended_agencies)\n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations for both models\n",
    "cosine_recommendations = generate_recommendations(test_brief_matrix, similarity_matrix_test, agency_names_test)\n",
    "\n",
    "# preparing the actual shrotlisted agencies\n",
    "actual_shortlisted_agencies_test = test_df.groupby('Brief Name')['Shortlisted Agency'].apply(list).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Recommendations Evaluation:\n",
      "Precision: 0.1571428571428572\n",
      "Recall: 0.7147108843537415\n",
      "F1-Score: 0.2576388792130527\n",
      "MAP: 0.9466269841269842\n",
      "NDCG: 0.7684759745738843\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the pre-defined evaluation function \n",
    "# Evaluating cosine recommendations\n",
    "precision_cosine = evaluate_recommendations(cosine_recommendations, actual_shortlisted_agencies_test)\n",
    "recall_cosine = calculate_recall(cosine_recommendations, actual_shortlisted_agencies_test)\n",
    "f1_score_cosine = calculate_f1_score(precision_cosine, recall_cosine)\n",
    "mean_avg_precision_cosine = calculate_mean_average_precision(cosine_recommendations, actual_shortlisted_agencies_test)\n",
    "ndcg_cosine = calculate_ndcg(cosine_recommendations, actual_shortlisted_agencies_test)\n",
    "\n",
    "print(\"Cosine Recommendations Evaluation:\")\n",
    "print(\"Precision:\", precision_cosine)\n",
    "print(\"Recall:\", recall_cosine)\n",
    "print(\"F1-Score:\", f1_score_cosine)\n",
    "print(\"MAP:\", mean_avg_precision_cosine)\n",
    "print(\"NDCG:\", ndcg_cosine)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. similarity metric calculation uisng jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(vector1, vector2):\n",
    "    intersection = np.logical_and(vector1, vector2).sum()\n",
    "    union = np.logical_or(vector1, vector2).sum()\n",
    "    return intersection / float(union)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_matrix(brief_matrix, agency_matrix):\n",
    "    num_briefs = brief_matrix.shape[0]\n",
    "    num_agencies = agency_matrix.shape[0]\n",
    "    similarity_matrix = np.zeros((num_briefs, num_agencies))\n",
    "    \n",
    "    for i in range(num_briefs):\n",
    "        for j in range(num_agencies):\n",
    "            similarity_matrix[i][j] = jaccard_similarity(brief_matrix.iloc[i].values, agency_matrix.iloc[j].values)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "jaccard_matrix_train = compute_jaccard_matrix(train_brief_matrix, train_agency_matrix)\n",
    "jaccard_matrix_test = compute_jaccard_matrix(test_brief_matrix, test_agency_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recommendation generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Recommendations for brief name: Acuity Magazine Podcast Series\n",
      "Agency: Ampel, Similarity Score: 1.0\n",
      "Agency: CreateFuture, Similarity Score: 0.38596491228070173\n",
      "Agency: Tobias, Similarity Score: 0.2698412698412698\n",
      "Agency: Salamandra.uk Animation Studios, Similarity Score: 0.2631578947368421\n",
      "Agency: off brand., Similarity Score: 0.26229508196721313\n",
      "Agency: Path Ventures, Similarity Score: 0.2542372881355932\n",
      "Agency: Blue Feathers, Similarity Score: 0.24193548387096775\n",
      "Agency: Akcelo, Similarity Score: 0.23333333333333334\n",
      "Agency: HeyBigMan!, Similarity Score: 0.22413793103448276\n",
      "Agency: OHMY, Similarity Score: 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "def get_single_recommendation_jaccard(brief_name, brief_features, similarity_matrix, agency_names, top_n=10):\n",
    "    brief_idx = np.where(brief_features.index == brief_name)[0][0]\n",
    "    similarity_scores = similarity_matrix[brief_idx]\n",
    "    similar_agencies_idx = similarity_scores.argsort()[::-1][:top_n]\n",
    "    recommended_agencies = [agency_names[idx] for idx in similar_agencies_idx]\n",
    "    recommended_scores = [similarity_scores[idx] for idx in similar_agencies_idx]\n",
    "    return recommended_agencies, recommended_scores\n",
    "\n",
    "# Example usage\n",
    "brief_name = test_brief_matrix.index[0]\n",
    "recommended_agencies, recommended_scores = get_single_recommendation_jaccard(brief_name, test_brief_matrix, jaccard_matrix_test, agency_names_test)\n",
    "print(\"Jaccard Recommendations for brief name:\", brief_name)\n",
    "for agency, score in zip(recommended_agencies, recommended_scores):\n",
    "    print(f\"Agency: {agency}, Similarity Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation generation for all the briefs in the test set \n",
    "def generate_recommendations(brief_features, similarity_matrix, agency_names, top_n=10):\n",
    "    recommendations = []\n",
    "    for brief_name in brief_features.index:\n",
    "        recommended_agencies, _ = get_single_recommendation_jaccard(brief_name, brief_features, similarity_matrix, agency_names, top_n)\n",
    "        recommendations.append(recommended_agencies)\n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations for both models\n",
    "jaccard_recommendations = generate_recommendations(test_brief_matrix, jaccard_matrix_test, agency_names_test)\n",
    "\n",
    "# preparing the actual shrotlisted agencies\n",
    "actual_shortlisted_agencies_test = test_df.groupby('Brief Name')['Shortlisted Agency'].apply(list).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Recommendations Evaluation:\n",
      "Precision: 0.15714285714285717\n",
      "Recall: 0.7206632653061226\n",
      "F1-Score: 0.2580229999584839\n",
      "MAP: 0.971513605442177\n",
      "NDCG: 0.7802931481229054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluating Jaccard recommendations\n",
    "precision_jaccard = evaluate_recommendations(jaccard_recommendations, actual_shortlisted_agencies_test)\n",
    "recall_jaccard = calculate_recall(jaccard_recommendations, actual_shortlisted_agencies_test)\n",
    "f1_score_jaccard = calculate_f1_score(precision_jaccard, recall_jaccard)\n",
    "mean_avg_precision_jaccard = calculate_mean_average_precision(jaccard_recommendations, actual_shortlisted_agencies_test)\n",
    "ndcg_jaccard = calculate_ndcg(jaccard_recommendations, actual_shortlisted_agencies_test)\n",
    "\n",
    "print(\"Jaccard Recommendations Evaluation:\")\n",
    "print(\"Precision:\", precision_jaccard)\n",
    "print(\"Recall:\", recall_jaccard)\n",
    "print(\"F1-Score:\", f1_score_jaccard)\n",
    "print(\"MAP:\", mean_avg_precision_jaccard)\n",
    "print(\"NDCG:\", ndcg_jaccard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3- Model with BERT EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating embeddings using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Convert the tensor to numpy array after taking the mean\n",
    "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agency_columns = [\n",
    "    \"Agency Skills\", \"Agency Industries\", \"Service Description\", \n",
    "    \"Agency Description\", \"Agency services\", \"Other Services\", \n",
    "    \"Vision\", \"Industry Experience\", \"Agency Locations\"\n",
    "]\n",
    "\n",
    "df['Agency_Profile'] = df[agency_columns].fillna('').apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "unique_agency_profiles = df.groupby('Shortlisted Agency')['Agency_Profile'].apply(' '.join).reset_index()\n",
    "\n",
    "brief_columns = [\"Brief Region\", \"Sub Brief Taxonomy\", \"Main Brief Taxonomy\"]\n",
    "\n",
    "df['Brief_Profile'] = df[brief_columns].fillna('').apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "unique_brief_profiles = df.groupby('Brief Name')['Brief_Profile'].apply(' '.join).reset_index()\n",
    "\n",
    "agency_embeddings = [get_embedding(profile) for profile in unique_agency_profiles['Agency_Profile']]\n",
    "brief_embeddings = [get_embedding(profile) for profile in unique_brief_profiles['Brief_Profile']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity calculation and train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split unique brief names and corresponding embeddings\n",
    "unique_brief_names = unique_brief_profiles['Brief Name'].unique()\n",
    "train_brief_names, test_brief_names = train_test_split(unique_brief_names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split unique agency names and corresponding embeddings\n",
    "unique_agency_names = unique_agency_profiles['Shortlisted Agency'].unique()\n",
    "\n",
    "# Extract the corresponding embeddings for train and test\n",
    "train_brief_embeddings = [brief_embeddings[i] for i in range(len(unique_brief_names)) if unique_brief_names[i] in train_brief_names]\n",
    "test_brief_embeddings = [brief_embeddings[i] for i in range(len(unique_brief_names)) if unique_brief_names[i] in test_brief_names]\n",
    "\n",
    "# Convert lists of numpy arrays to numpy array for similarity computation\n",
    "train_brief_embeddings = np.vstack(train_brief_embeddings)\n",
    "test_brief_embeddings = np.vstack(test_brief_embeddings)\n",
    "agency_embeddings = np.vstack(agency_embeddings)\n",
    "\n",
    "# Calculate similarity matrices for training and test data\n",
    "similarity_matrix_train = cosine_similarity(train_brief_embeddings, agency_embeddings)\n",
    "similarity_matrix_test = cosine_similarity(test_brief_embeddings, agency_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recommenation generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_recommendation(brief_name, test_brief_names, similarity_matrix, agency_names, top_n=10):\n",
    "    # Find the index of the brief name\n",
    "    brief_idx = np.where(test_brief_names == brief_name)[0][0]\n",
    "\n",
    "    # Get similarity scores and recommended agency indexes\n",
    "    similarity_scores = similarity_matrix[brief_idx]\n",
    "    similar_agencies_idx = similarity_scores.argsort()[::-1][:top_n]\n",
    "\n",
    "    # Get the names and scores of the recommended agencies\n",
    "    recommended_agencies = [agency_names[idx] for idx in similar_agencies_idx]\n",
    "    recommended_scores = [similarity_scores[idx] for idx in similar_agencies_idx]\n",
    "\n",
    "    return recommended_agencies, recommended_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for brief name: MyAviva Engagement\n",
      "Agency: Neu, Similarity Score: 0.8752783536911011\n",
      "Agency: Fun Agency Ltd, Similarity Score: 0.8726052045822144\n",
      "Agency: Dark Blue, Similarity Score: 0.8697119951248169\n",
      "Agency: Oxara London, Similarity Score: 0.8680103421211243\n",
      "Agency: Experience, Similarity Score: 0.8678798079490662\n",
      "Agency: Apadmi, Similarity Score: 0.8658484816551208\n",
      "Agency: Solace Digital, Similarity Score: 0.8574975728988647\n",
      "Agency: Blicx, Similarity Score: 0.8560953140258789\n",
      "Agency: Tinker Taylor, Similarity Score: 0.8523766994476318\n",
      "Agency: InspoHub Ltd, Similarity Score: 0.8509337902069092\n"
     ]
    }
   ],
   "source": [
    "brief_name = test_brief_names[0]\n",
    "recommended_agencies, recommended_scores = get_single_recommendation(brief_name, test_brief_names, similarity_matrix_test, unique_agency_names)\n",
    "\n",
    "print(\"Recommendations for brief name:\", brief_name)\n",
    "for agency, score in zip(recommended_agencies, recommended_scores):\n",
    "    print(f\"Agency: {agency}, Similarity Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(test_brief_names, similarity_matrix, agency_names, top_n=10):\n",
    "    recommendations = []\n",
    "    for brief_name in test_brief_names:\n",
    "        recommended_agencies, _ = get_single_recommendation(brief_name, test_brief_names, similarity_matrix, agency_names, top_n)\n",
    "        recommendations.append(recommended_agencies)\n",
    "    return recommendations\n",
    "actual_shortlisted_agencies_test = df[df['Brief Name'].isin(test_brief_names)].groupby('Brief Name')['Shortlisted Agency'].apply(list).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.02142857142857143\n",
      "Test Recall: 0.09523809523809523\n",
      "Test F1-Score: 0.034985422740524776\n",
      "Test MAP: 0.0790107709750567\n",
      "Test NDCG: 0.06420332910240766\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations for test briefs using BERT embeddings\n",
    "recommendations_test = generate_recommendations(test_brief_names, similarity_matrix_test, unique_agency_names)\n",
    "\n",
    "# Compute the evaluation metrics using the functions you provided\n",
    "precision_emb = evaluate_recommendations(recommendations_test, actual_shortlisted_agencies_test)\n",
    "recall_emb = calculate_recall(recommendations_test, actual_shortlisted_agencies_test)\n",
    "f1_score_emb = calculate_f1_score(precision_emb, recall_emb)\n",
    "mean_avg_precision_emb = calculate_mean_average_precision(recommendations_test, actual_shortlisted_agencies_test)\n",
    "ndcg_emb = calculate_ndcg(recommendations_test, actual_shortlisted_agencies_test)\n",
    "\n",
    "print(\"Test Precision:\", precision_emb)\n",
    "print(\"Test Recall:\", recall_emb)\n",
    "print(\"Test F1-Score:\", f1_score_emb)\n",
    "print(\"Test MAP:\", mean_avg_precision_emb)\n",
    "print(\"Test NDCG:\", ndcg_emb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
